{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In order to train a machine learning model to classify text, we need:\n",
    "1. a way to preprocess text\n",
    "2. a label for each text\n",
    "3. a way to represent each text as vector input\n",
    "4. a model to learn  a function $f(input) = label$\n",
    "5. a way to evaluate how well the model works\n",
    "6. a way to predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist: how to classify my data\n",
    "\n",
    "1. label at ***least 2000*** instances in your data set\n",
    "2. preprocess the text of *all* instances in your data (labeled and unlabeled)\n",
    "3. read in the labeled instances and their labels\n",
    "4. use `TfidfVectorizer` to extract the features and transform them into feature vectors\n",
    "5. select the top $N$ features (where $N$ is smaller than the number of labeled instances)\n",
    "6. use 5-fold CV to find the best regularization parameter, top $N$ feature selection, and maybe feature generation and preprocessing steps\n",
    "7. create a classifier with the best settings\n",
    "\n",
    "Once you are satisfied with the results:\n",
    "\n",
    "8. read in the rest of the (unlabeled) instances\n",
    "9. use the `TfidfVectorizer` from 4. to transform the new data into vectors\n",
    "10. use the `SelectKBest` selector from 5. to get the top $N$ features\n",
    "11. use the classifier from 7. to predict the labels for the new data\n",
    "12. save the predicted labels or probabilities to your database or an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:30:14.819861Z",
     "start_time": "2021-04-28T08:29:58.305666Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "countries = {'Italy', 'France', 'Spain', 'Germany', 'US'}\n",
    "wine = pd.read_excel('../data/wine_reviews.xlsx')\n",
    "data = wine[wine.country.isin(countries)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying new (**held-out**) data is called **prediction**. We reuse the weights we have learned before on a new data matrix to predict the new outcomes.\n",
    "\n",
    "Important: the new data needs to have the same number of features, and undergo the same preprocessing! The best way to ensure this is to make data splits after the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:30:34.160922Z",
     "start_time": "2021-04-28T08:30:34.098091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data\n",
    "data = data[data['description_cleaned'].isna() == False]\n",
    "data = data.sample(frac=1)[:20000]\n",
    "\n",
    "# determine the size of training, develpment and test set:\n",
    "N = len(data)\n",
    "train_size = int(N*0.5)\n",
    "dev_size = int(N*0.25)\n",
    "test_size = int(N*0.25)\n",
    "\n",
    "# split the data into training, develpment and test set:\n",
    "train = data[:train_size]\n",
    "dev = data[train_size: train_size+dev_size]\n",
    "test = data[train_size+dev_size:]\n",
    "print(len(train), len(dev), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:30:37.238717Z",
     "start_time": "2021-04-28T08:30:37.218770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>description_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97304</th>\n",
       "      <td>97304</td>\n",
       "      <td>97307</td>\n",
       "      <td>France</td>\n",
       "      <td>This is a smooth, orange- and peach-flavored w...</td>\n",
       "      <td>Sainte-Fleur</td>\n",
       "      <td>87</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Vin de Pays Var</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Viognier</td>\n",
       "      <td>Triennes</td>\n",
       "      <td>be smooth orange- peach flavor wine rich almos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114050</th>\n",
       "      <td>114050</td>\n",
       "      <td>114054</td>\n",
       "      <td>US</td>\n",
       "      <td>With only 72% Zin, they can't call it by the v...</td>\n",
       "      <td>The Imposter</td>\n",
       "      <td>89</td>\n",
       "      <td>32.0</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>California Other</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>JC Cellars</td>\n",
       "      <td>only can not call varietal name be fine brisk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109744</th>\n",
       "      <td>109744</td>\n",
       "      <td>109747</td>\n",
       "      <td>US</td>\n",
       "      <td>Brisk and ripe, this Chard, from a cool region...</td>\n",
       "      <td>Wolff Vineyard</td>\n",
       "      <td>88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Edna Valley</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Mount Eden</td>\n",
       "      <td>brisk ripe cool region show classic profile pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129211</th>\n",
       "      <td>129211</td>\n",
       "      <td>129215</td>\n",
       "      <td>US</td>\n",
       "      <td>Too soft for comfort. Lacks structural integri...</td>\n",
       "      <td>Caton Vineyard</td>\n",
       "      <td>84</td>\n",
       "      <td>44.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Petite Sirah</td>\n",
       "      <td>Ty Caton</td>\n",
       "      <td>too soft comfort lack structural integrity liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62085</th>\n",
       "      <td>62085</td>\n",
       "      <td>62087</td>\n",
       "      <td>US</td>\n",
       "      <td>A good effort, with clean varietal character: ...</td>\n",
       "      <td>Vie!</td>\n",
       "      <td>87</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Viognier</td>\n",
       "      <td>Vin du Lac</td>\n",
       "      <td>good effort clean varietal character lemon lim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1 country  \\\n",
       "97304        97304         97307  France   \n",
       "114050      114050        114054      US   \n",
       "109744      109744        109747      US   \n",
       "129211      129211        129215      US   \n",
       "62085        62085         62087      US   \n",
       "\n",
       "                                              description     designation  \\\n",
       "97304   This is a smooth, orange- and peach-flavored w...    Sainte-Fleur   \n",
       "114050  With only 72% Zin, they can't call it by the v...    The Imposter   \n",
       "109744  Brisk and ripe, this Chard, from a cool region...  Wolff Vineyard   \n",
       "129211  Too soft for comfort. Lacks structural integri...  Caton Vineyard   \n",
       "62085   A good effort, with clean varietal character: ...            Vie!   \n",
       "\n",
       "        points  price    province              region_1          region_2  \\\n",
       "97304       87   20.0    Provence       Vin de Pays Var               NaN   \n",
       "114050      89   32.0  California            California  California Other   \n",
       "109744      88   18.0  California           Edna Valley     Central Coast   \n",
       "129211      84   44.0  California         Sonoma Valley            Sonoma   \n",
       "62085       87   18.0  Washington  Columbia Valley (WA)   Columbia Valley   \n",
       "\n",
       "             variety      winery  \\\n",
       "97304       Viognier    Triennes   \n",
       "114050     Red Blend  JC Cellars   \n",
       "109744    Chardonnay  Mount Eden   \n",
       "129211  Petite Sirah    Ty Caton   \n",
       "62085       Viognier  Vin du Lac   \n",
       "\n",
       "                                      description_cleaned  \n",
       "97304   be smooth orange- peach flavor wine rich almos...  \n",
       "114050  only can not call varietal name be fine brisk ...  \n",
       "109744  brisk ripe cool region show classic profile pu...  \n",
       "129211  too soft comfort lack structural integrity liv...  \n",
       "62085   good effort clean varietal character lemon lim...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually splitting the data, you can also use `sklearn`'s built-in `StratifiedKFold` or `StratifiedShuffleSplit`, which ensures equal proportions of labels in all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:31:03.696247Z",
     "start_time": "2021-04-28T08:31:03.686273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97304     France\n",
      "114050        US\n",
      "109744        US\n",
      "129211        US\n",
      "62085         US\n",
      "110443        US\n",
      "93961     France\n",
      "132702        US\n",
      "44148         US\n",
      "26010         US\n",
      "Name: country, dtype: object\n"
     ]
    }
   ],
   "source": [
    "target = 'country'\n",
    "\n",
    "y_train = train[target]\n",
    "\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:31:28.614356Z",
     "start_time": "2021-04-28T08:31:28.601391Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform dev and test with the same label converter\n",
    "y_dev = dev[target]\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the classes (and their mapping to an intger) from the fitted classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:31:46.299047Z",
     "start_time": "2021-04-28T08:31:46.282056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'France': 0.1857,\n",
       " 'US': 0.5255,\n",
       " 'Italy': 0.201,\n",
       " 'Spain': 0.0686,\n",
       " 'Germany': 0.0192}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "{k: v/len(y_train) for k, v in Counter(y_train).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:31:53.715928Z",
     "start_time": "2021-04-28T08:31:53.706648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': 0.5354,\n",
       " 'France': 0.1812,\n",
       " 'Spain': 0.0708,\n",
       " 'Italy': 0.1926,\n",
       " 'Germany': 0.02}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v/len(y_dev) for k, v in Counter(y_dev).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:32:00.002542Z",
     "start_time": "2021-04-28T08:31:59.988580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': 0.5408,\n",
       " 'Italy': 0.1862,\n",
       " 'Germany': 0.0212,\n",
       " 'France': 0.175,\n",
       " 'Spain': 0.0768}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v/len(y_test) for k, v in Counter(y_test).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the Input\n",
    "\n",
    "## Bags of words\n",
    "\n",
    "The easiest way is to represent features is as a counts of all words in the text. It takes two steps:\n",
    "1. collect the counts for each word\n",
    "2. transform the individual counts into one big matrix\n",
    "\n",
    "The result is a matrix $X$ with one row for each instance, and one column for each word in the vocabulary.\n",
    "\n",
    "![Bag of words procedure](bow.png)\n",
    "\n",
    "We can use the `TfidfVectorizer` object to get the frequency of each word, weighted by the number of documents it occurs in (that tempers the influence of freuqent, but uninformative words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:33:37.393225Z",
     "start_time": "2021-04-28T08:33:36.965234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4922)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             min_df=0.001, \n",
    "                             max_df=0.7, \n",
    "                             analyzer='word',\n",
    "                             sublinear_tf=True\n",
    "                            )\n",
    "\n",
    "X_train = vectorizer.fit_transform(train['description_cleaned'])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:33:37.741640Z",
     "start_time": "2021-04-28T08:33:37.394256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4922) (5000, 4922)\n"
     ]
    }
   ],
   "source": [
    "X_dev = vectorizer.transform(dev['description_cleaned'])\n",
    "X_test = vectorizer.transform(test['description_cleaned'])\n",
    "print(X_dev.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Baseline\n",
    "\n",
    "So, is that performance good? Let's compare to a **baseline**, i.e., a null-hypothesis. The simplest one is that all instances belong to the most frequnt class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:33:38.429722Z",
     "start_time": "2021-04-28T08:33:38.368990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.00      0.00      0.00       906\n",
      "     Germany       0.00      0.00      0.00       100\n",
      "       Italy       0.00      0.00      0.00       963\n",
      "       Spain       0.00      0.00      0.00       354\n",
      "          US       0.54      1.00      0.70      2677\n",
      "\n",
      "    accuracy                           0.54      5000\n",
      "   macro avg       0.11      0.20      0.14      5000\n",
      "weighted avg       0.29      0.54      0.37      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiziano\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# find the most frequent class in the training data\n",
    "most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "most_frequent.fit(X_train, y_train)\n",
    "\n",
    "# get the performance on the development set\n",
    "dumb_predictions = most_frequent.predict(X_dev)\n",
    "\n",
    "print(classification_report(y_dev, dumb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:33:45.711845Z",
     "start_time": "2021-04-28T08:33:43.956529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.7 s\n",
      "LogisticRegression(n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs')\n",
    "%time classifier.fit(X_train, y_train)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the classes and their order/integer ID, you can use the `classes_` property of the fitted classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:33:55.616164Z",
     "start_time": "2021-04-28T08:33:55.606155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Germany', 'Italy', 'Spain', 'US'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the performance of this classifier on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:34:01.032017Z",
     "start_time": "2021-04-28T08:34:00.942344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US' 'US' 'US' 'US' 'US' 'Italy' 'Italy' 'US' 'US' 'US']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.94      0.83      0.88       906\n",
      "     Germany       0.81      0.17      0.28       100\n",
      "       Italy       0.92      0.91      0.92       963\n",
      "       Spain       0.91      0.64      0.75       354\n",
      "          US       0.87      0.97      0.92      2677\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.70      0.75      5000\n",
      "weighted avg       0.90      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_dev)\n",
    "print(predictions[:10])\n",
    "print(classification_report(y_dev,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can also predict the probabilities of belonging to each class. Here, we get  a distribution over classes, i.e., each column is the probability of one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:34:26.942422Z",
     "start_time": "2021-04-28T08:34:26.928461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France\tGermany\tItaly\tSpain\tUS\n",
      "[[0.01807795 0.00959097 0.0011403  0.00397612 0.96721465]\n",
      " [0.37931576 0.02323042 0.04177256 0.02646828 0.52921299]\n",
      " [0.04119463 0.00671186 0.32547875 0.01573383 0.61088093]\n",
      " [0.0058189  0.00476416 0.0083631  0.01077653 0.97027731]\n",
      " [0.05723127 0.003991   0.02552457 0.00773765 0.90551551]\n",
      " [0.07520709 0.02252842 0.45355449 0.04365028 0.40505972]\n",
      " [0.05181938 0.01666117 0.43616734 0.29711717 0.19823494]\n",
      " [0.24356426 0.06592036 0.07192926 0.05790301 0.56068311]\n",
      " [0.01373661 0.00269708 0.04534902 0.01891362 0.91930366]\n",
      " [0.14923503 0.00509665 0.02729542 0.00792077 0.81045213]]\n"
     ]
    }
   ],
   "source": [
    "probabilities = classifier.predict_proba(X_dev)\n",
    "print('\\t'.join(classifier.classes_))\n",
    "print(probabilities[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict()` function uses a threshold of $0.5$ to assign binary labels, or the argmax for multiple classes. If you want to emphasize precision more, you can base the label on a higher threshold. If you want to emphasize recall, you can base the label on a lower threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting better\n",
    "\n",
    "If we do not like the results, there are several parameters we can experiment with:\n",
    "1. class balance\n",
    "2. regularization\n",
    "3. feature selection\n",
    "4. dimensionality reduction\n",
    "5. different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Class balance\n",
    "\n",
    "We can weigh each class inversely proportional to its frequency, i.e., assign higher weight to rarer classes, to improve performance on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:57:18.357749Z",
     "start_time": "2021-04-28T08:57:16.569326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.69 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.88      0.88       906\n",
      "     Germany       0.44      0.75      0.56       100\n",
      "       Italy       0.88      0.93      0.91       963\n",
      "       Spain       0.74      0.89      0.81       354\n",
      "          US       0.96      0.89      0.92      2677\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.78      0.87      0.82      5000\n",
      "weighted avg       0.91      0.89      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_balanced = LogisticRegression(n_jobs=-1, \n",
    "                                         multi_class='auto', \n",
    "                                         solver='lbfgs', \n",
    "                                         class_weight='balanced' # added\n",
    "                                         \n",
    "                                        )\n",
    "%time classifier_balanced.fit(X_train, y_train)\n",
    "predictions_balanced = classifier_balanced.predict(X_dev)\n",
    "\n",
    "print(classification_report(y_dev, predictions_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization strength\n",
    "Typically, performance is lower on unseen data, because our model **overfit** the training data: it expects the new data to look *exactly* the same as the training data. That is almost never true.\n",
    "\n",
    "In order to prevent the model from overfitting, we need to **regularize** it. Essentially, we make it harder to learn the training data.\n",
    "\n",
    "It makes sense to force the model to spread the weights more evenly over all features, rather than bet on a few feature, which mighht not be present in future data.\n",
    "\n",
    "We can do this by training the model with the `C` parameter of the L2 regularization. The default is `1`. Lower values mean stricter regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:58:07.642594Z",
     "start_time": "2021-04-28T08:57:57.045076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "New best performance: 0.912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.89      0.89       906\n",
      "     Germany       0.62      0.60      0.61       100\n",
      "       Italy       0.90      0.93      0.92       963\n",
      "       Spain       0.84      0.85      0.85       354\n",
      "          US       0.95      0.93      0.94      2677\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.84      0.84      0.84      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "\n",
      "20\n",
      "New best performance: 0.9138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.90      0.89      0.89       906\n",
      "     Germany       0.59      0.62      0.60       100\n",
      "       Italy       0.90      0.94      0.92       963\n",
      "       Spain       0.84      0.86      0.85       354\n",
      "          US       0.95      0.93      0.94      2677\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.83      0.85      0.84      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "\n",
      "10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.89      0.89       906\n",
      "     Germany       0.58      0.67      0.62       100\n",
      "       Italy       0.90      0.94      0.92       963\n",
      "       Spain       0.82      0.86      0.84       354\n",
      "          US       0.95      0.93      0.94      2677\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.83      0.86      0.84      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "\n",
      "5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.89      0.89       906\n",
      "     Germany       0.55      0.69      0.61       100\n",
      "       Italy       0.90      0.94      0.92       963\n",
      "       Spain       0.80      0.87      0.84       354\n",
      "          US       0.95      0.92      0.94      2677\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.82      0.86      0.84      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "\n",
      "2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.89      0.89       906\n",
      "     Germany       0.51      0.73      0.60       100\n",
      "       Italy       0.90      0.94      0.92       963\n",
      "       Spain       0.76      0.89      0.82       354\n",
      "          US       0.96      0.91      0.93      2677\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.80      0.87      0.83      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "\n",
      "0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.86      0.88       906\n",
      "     Germany       0.38      0.78      0.51       100\n",
      "       Italy       0.88      0.92      0.90       963\n",
      "       Spain       0.71      0.90      0.79       354\n",
      "          US       0.96      0.88      0.92      2677\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.76      0.87      0.80      5000\n",
      "weighted avg       0.90      0.88      0.89      5000\n",
      "\n",
      "\n",
      "0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.87      0.83      0.85       906\n",
      "     Germany       0.29      0.85      0.43       100\n",
      "       Italy       0.86      0.92      0.89       963\n",
      "       Spain       0.63      0.87      0.73       354\n",
      "          US       0.96      0.83      0.89      2677\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.72      0.86      0.76      5000\n",
      "weighted avg       0.89      0.85      0.86      5000\n",
      "\n",
      "\n",
      "0.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.85      0.83      0.84       906\n",
      "     Germany       0.25      0.90      0.39       100\n",
      "       Italy       0.85      0.91      0.88       963\n",
      "       Spain       0.61      0.86      0.72       354\n",
      "          US       0.96      0.80      0.87      2677\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.70      0.86      0.74      5000\n",
      "weighted avg       0.88      0.83      0.84      5000\n",
      "\n",
      "\n",
      "0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.78      0.82      0.80       906\n",
      "     Germany       0.18      0.91      0.30       100\n",
      "       Italy       0.83      0.89      0.86       963\n",
      "       Spain       0.56      0.85      0.68       354\n",
      "          US       0.97      0.71      0.82      2677\n",
      "\n",
      "    accuracy                           0.78      5000\n",
      "   macro avg       0.66      0.84      0.69      5000\n",
      "weighted avg       0.86      0.78      0.80      5000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "best_c = None\n",
    "best_performance = 0.0\n",
    "\n",
    "for c in [50, 20, 10, 5, 2, 0.5, 0.1, 0.05, 0.01]:\n",
    "    print(c)\n",
    "    classifier_c = LogisticRegression(n_jobs=-1, \n",
    "                                      multi_class='auto', \n",
    "                                      solver='lbfgs',\n",
    "                                      class_weight='balanced',\n",
    "                                      C=c\n",
    "                                     )\n",
    "    \n",
    "    classifier_c.fit(X_train, y_train)\n",
    "    predictions_c = classifier_c.predict(X_dev)\n",
    "    score = f1_score(y_dev, predictions_c, average='micro')\n",
    "    if score > best_performance:\n",
    "        best_performance = score\n",
    "        best_c = c\n",
    "        print(\"New best performance: {}\".format(score))\n",
    "        \n",
    "    print(classification_report(y_dev, predictions_c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a manual search, we can also use a grid search over all classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T08:59:28.843849Z",
     "start_time": "2021-04-28T08:58:34.021871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(n_jobs=-1),\n",
       "             param_grid={'C': [20, 10, 5, 1, 0.01],\n",
       "                         'class_weight': ['balanced', None]},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# setting up the classifier we want to optimize\n",
    "base_clf = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# defining parameters to optimize\n",
    "param_grid = {'C': [20, 10, 5, 1, 0.01],\n",
    "              'class_weight': ['balanced', None]\n",
    "             }\n",
    "# run the optimization\n",
    "search = GridSearchCV(base_clf, # use the classifier defined above\n",
    "                      param_grid, # use the parameters defined above\n",
    "                      cv=5, # use 5-fold cross validation\n",
    "                      scoring='f1_micro') # use micro F1 to select best model\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the winning combination of parameters: we access the `best_estimator_` property of the grid search object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:25:25.668545Z",
     "start_time": "2021-04-28T10:25:21.281388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': -1, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight='balanced', n_jobs=-1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_best = search.best_estimator_\n",
    "print(clf_best.get_params(), search.best_score_)\n",
    "\n",
    "# fit this classifier on the entire training data, instead of CV\n",
    "clf_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n",
    "\n",
    "Not all features are helpful. Let's select the top $k$ based on how well they predict they outcome of the training data.\n",
    "\n",
    "We use two libraries from `sklearn`, `SelectKBest` (the selection algorithm) and `chi2` (the selection criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:25:41.347371Z",
     "start_time": "2021-04-28T10:25:30.917394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9040000000000001 SelectKBest(k=4500, score_func=<function chi2 at 0x000002B09E239F70>)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set up the sequence of steps\n",
    "pipe = Pipeline([\n",
    "    ('selector', 'passthrough'), # feature selection\n",
    "    ('classifier', clf_best) # the classifier\n",
    "])\n",
    "\n",
    "# specify selection range\n",
    "param_grid = [\n",
    "    {\n",
    "        'selector': [SelectKBest(chi2)],\n",
    "        'selector__k': [4500, 4000, 2000, 1000, 500]\n",
    "    },\n",
    "]\n",
    "\n",
    "# fit the model to different feature sets\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=5, \n",
    "                    scoring='f1_micro',\n",
    "                    n_jobs=-1,\n",
    "                   )\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_, grid.best_params_['selector'])\n",
    "\n",
    "# save the best selector\n",
    "selector = grid.best_params_['selector'].fit(X_train, y_train)\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_dev_sel = selector.transform(X_dev)\n",
    "X_test_sel = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:25:44.548249Z",
     "start_time": "2021-04-28T10:25:41.349365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      France       0.89      0.88      0.88       906\n",
      "     Germany       0.43      0.75      0.55       100\n",
      "       Italy       0.89      0.93      0.91       963\n",
      "       Spain       0.74      0.89      0.81       354\n",
      "          US       0.96      0.89      0.93      2677\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.78      0.87      0.81      5000\n",
      "weighted avg       0.91      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_sel = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', \n",
    "                                    class_weight='balanced')\n",
    "classifier_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "predictions_sel = classifier_sel.predict(X_dev_sel)\n",
    "print(classification_report(y_dev, predictions_sel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:26:33.651564Z",
     "start_time": "2021-04-28T10:25:48.159947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8706000000000002 TruncatedSVD(n_components=400)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# set up the sequence of steps\n",
    "pipe = Pipeline([\n",
    "    ('reduction', 'passthrough'),\n",
    "    ('classifier', clf_best)\n",
    "])\n",
    "\n",
    "# specify selection range\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduction': [TruncatedSVD()],\n",
    "        'reduction__n_components': [400, 300, 200, 100]\n",
    "    },\n",
    "]\n",
    "\n",
    "# fit the model to different feature sets\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=5, \n",
    "                    scoring='f1_micro',\n",
    "                    n_jobs=-1,\n",
    "                   )\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_, grid.best_params_['reduction'])\n",
    "\n",
    "# save the best selector\n",
    "reductor = grid.best_params_['reduction'].fit(X_train, y_train)\n",
    "X_train_sel = reductor.transform(X_train)\n",
    "X_dev_sel = reductor.transform(X_dev)\n",
    "X_test_sel = reductor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting insights\n",
    "\n",
    "The fitted model has coefficients (weights, betas) for each word/feature in our vocabulary.\n",
    "\n",
    "To find out which features are most indicative for each class, we need some code to map back from the coefficient to the corresponding feature.\n",
    "\n",
    "If we reduced the number of features, we need to take that into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T10:26:34.552600Z",
     "start_time": "2021-04-28T10:26:33.656202Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2348d4bf6af6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# get class with highest weight for each feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtop_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# make DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_clf' is not defined"
     ]
    }
   ],
   "source": [
    "# get the names of the features\n",
    "features = vectorizer.get_feature_names()\n",
    "num_feats = len(features)\n",
    "\n",
    "# get the indices of the selection\n",
    "top_scores = selector.scores_.argsort()[-num_feats:]\n",
    "\n",
    "# sort feature names\n",
    "best_indicator_terms = [features[i] for i in sorted(top_scores)] \n",
    "\n",
    "# get class with highest weight for each feature\n",
    "top_class = [best_clf.classes_[c] for c in best_clf.coef_.argmax(axis=0)]\n",
    "\n",
    "# make DataFrame\n",
    "top_indicator_scores = pd.DataFrame(data={'feature': best_indicator_terms, \n",
    "                                          'class': top_class,\n",
    "                                          'coefficient': best_clf.coef_.max(axis=0)})\n",
    "\n",
    "# sort in descending order\n",
    "top_indicator_scores.sort_values('coefficient', ascending=False, inplace=True)\n",
    "top_indicator_scores.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def bootstrap_sample(system1, system2, gold, samples=1000, score=f1_score, average='micro'):\n",
    "    \"\"\"\n",
    "    compute the proportion of times the performance difference of the \n",
    "    two systems on a subsample is significantly different from the \n",
    "    performance on the entire sample\n",
    "    \"\"\"\n",
    "    N = len(gold) # number of instances\n",
    "    \n",
    "    # make sure the two systems have the same number of samples\n",
    "    assert len(system1) == N and len(system2) == N, 'samples have different lengths'\n",
    "\n",
    "    # compute performance score on entire sample\n",
    "    base_score1 = score(gold, system1, average=average)\n",
    "    base_score2 = score(gold, system2, average=average)\n",
    "    print(\"Base difference: {} vs. {}\".format(base_score1, base_score2))\n",
    "\n",
    "    # switch systems if system2 is better\n",
    "    if base_score2 > base_score1:\n",
    "        system1, system2 = system2, system1\n",
    "        base_score1, base_score2 = base_score2, base_score1\n",
    "    \n",
    "    # compute the difference\n",
    "    basedelta = base_score1 - base_score2\n",
    "    assert basedelta > 0, 'Wrong system first, system1 needs to be better!'\n",
    "\n",
    "    system1 = np.array(system1)\n",
    "    system2 = np.array(system2)\n",
    "    gold = np.array(gold)\n",
    "\n",
    "    p = 0\n",
    "    deltas = []\n",
    "    for i in range(samples):\n",
    "        # select a subsample, with replacement\n",
    "        sample = np.random.choice(N, size=N, replace=True)\n",
    "\n",
    "        # collect data corresponding to subsample\n",
    "        sample1 = system1[sample]\n",
    "        sample2 = system2[sample]\n",
    "        gold_sample = gold[sample]\n",
    "\n",
    "        # compute scores on subsample\n",
    "        sample_score1 = score(gold_sample, sample1, average=average)\n",
    "        sample_score2 = score(gold_sample, sample2, average=average)\n",
    "        sample_delta = sample_score1 - sample_score2\n",
    "\n",
    "        # check whether the observed sample difference is at least \n",
    "        # twice as large as the base difference\n",
    "        if sample_delta > 2*basedelta:\n",
    "            p += 1\n",
    "        deltas.append(sample_delta)\n",
    "                \n",
    "    return p/samples, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base difference: 0.8908000000000001 vs. 0.5226\n",
      "0.0 True\n"
     ]
    }
   ],
   "source": [
    "p_value, deltas = bootstrap_sample(predictions, dumb_predictions, y_dev)\n",
    "print(p_value, p_value < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR3klEQVR4nO3dfbAddX3H8fenRISgDmACRSAGLIMioyNerRVLtZEWRQnoWHFqGxGNdKhP7UwN1hFnOszEqfWh01qNokZFbMAHaKkK0qrjH4JBbAUCBSVCJJL4VHxgwOC3f5zNeok35OSes2dvct+vmTtn97d77n7PLyf55Ld79ndSVUiSBPBbfRcgSZo7DAVJUstQkCS1DAVJUstQkCS1FvRdwCgWLVpUS5cu7bsMSdqjXHfddT+oqsUzbdujQ2Hp0qWsX7++7zIkaY+S5Ls72+bpI0lSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSq7M7mpN8CHgBsKWqjm/a/h54IXA/8G3grKr6SbPtPOBs4AHgdVX1ha5qkyZh6aorZv3cjatPHWMl0vC6HCl8BDhlh7argOOr6knA/wLnASQ5DjgTeGLznPcm2afD2iRJM+gsFKrqK8CPdmi7sqq2NatfA45olpcDn6yq+6rqduA24Old1SZJmlmf1xReCXyuWT4cuHPatk1N229IsjLJ+iTrt27d2nGJkjS/9BIKSf4W2AZctL1pht1qpudW1ZqqmqqqqcWLZ5z5VZI0SxOfOjvJCgYXoJdV1fZ/+DcBR07b7QjgrknXpr3PKBd7pflooiOFJKcAbwJOq6pfTNt0OXBmkocnOQo4Brh2krVJkrr9SOrFwLOBRUk2Aecz+LTRw4GrkgB8rarOqaobk6wDbmJwWuncqnqgq9okSTPrLBSq6mUzNF/4EPtfAFzQVT2SpF3zjmZJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUmviX7Ij7S6/KEeaHEcKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWdzRLc9Aod3FvXH3qGCvRfONIQZLU6iwUknwoyZYkN0xrOzjJVUlubR4PmrbtvCS3JbklyR93VZckaee6HCl8BDhlh7ZVwNVVdQxwdbNOkuOAM4EnNs95b5J9OqxNkjSDzkKhqr4C/GiH5uXA2mZ5LXD6tPZPVtV9VXU7cBvw9K5qkyTNbNLXFA6tqs0AzeMhTfvhwJ3T9tvUtP2GJCuTrE+yfuvWrZ0WK0nzzVy50JwZ2mqmHatqTVVNVdXU4sWLOy5LkuaXSYfC3UkOA2getzTtm4Ajp+13BHDXhGuTpHlv0qFwObCiWV4BXDat/cwkD09yFHAMcO2Ea5Okea+zm9eSXAw8G1iUZBNwPrAaWJfkbOAO4CUAVXVjknXATcA24NyqeqCr2iRJM+ssFKrqZTvZtGwn+18AXNBVPZKkXZsrF5olSXOAoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqRWL6GQ5I1JbkxyQ5KLk+yX5OAkVyW5tXk8qI/aJGk+WzDpAyY5HHgdcFxV3ZtkHXAmcBxwdVWtTrIKWAW8adL1SXu6pauumPVzN64+dYyVaE/U1+mjBcD+SRYAC4G7gOXA2mb7WuD0fkqTpPlr4iOFqvpekncAdwD3AldW1ZVJDq2qzc0+m5McMtPzk6wEVgIsWbJkUmVrRKP871XS5Ex8pNBcK1gOHAU8BjggycuHfX5VramqqaqaWrx4cVdlStK81Mfpo+cCt1fV1qr6JfBp4JnA3UkOA2get/RQmyTNa32Ewh3AM5IsTBJgGbABuBxY0eyzArish9okaV4b6ppCkuOr6oZxHLCqrklyKfANYBtwPbAGeASwLsnZDILjJeM4niRpeMNeaH5fkn2BjwCfqKqfjHLQqjofOH+H5vsYjBokST0Z6vRRVT0L+FPgSGB9kk8kObnTyiRJEzf0NYWquhV4C4Mbyv4A+MckNyd5UVfFSZIma6hQSPKkJO9icEH4D4EXVtUTmuV3dVifJGmChr2m8E/AB4A3V9W92xur6q4kb+mkMknSxA0bCs8H7q2qBwCS/BawX1X9oqo+1ll1kqSJGvaawheB/aetL2zaJEl7kWFDYb+q+tn2lWZ5YTclSZL6Mmwo/DzJCdtXkjyVwWR2kqS9yLDXFN4AXJLkrmb9MOClnVQkSerNUKFQVV9P8njgWCDAzc1kdpKkvcjufJ/C04ClzXOekoSq+mgnVUmSejHshHgfAx4HfBN4oGkuwFCQpL3IsCOFKQbfqVxdFiNJ6tewnz66AfjtLguRJPVv2JHCIuCmJNcymOIagKo6rZOqJEm9GDYU3tZlEZKkuWHYj6R+OcljgWOq6otJFgL7dFuaJGnShp06+9XApcD7m6bDgc92VJMkqSfDXmg+FzgRuAfaL9w5pKuiJEn9GDYU7quq+7evJFnA4D4FSdJeZNhQ+HKSNwP7N9/NfAnwb92VJUnqw7ChsArYCnwLeA3wHwy+r1mStBcZ9tNHv2LwdZwf6LYcSVKfhp376HZmuIZQVUePvSLNWUtXXdF3CZI6tjtzH223H/AS4ODxlyNJ6tOwp49+uEPTu5N8FXjrbA6a5EDgg8DxDEYgrwRuAf6VwfTcG4E/qaofz+b3S5qdUUeDG1efOqZK1Jdhb147YdrPVJJzgEeOcNz3AJ+vqscDTwY2MLiYfXVVHQNc3axLkiZo2NNH/zBteRvN/+Rnc8AkjwJOAl4B0Nz/cH+S5cCzm93WAl8C3jSbY0iSZmfY00fPGeMxj2bw8dYPJ3kycB3weuDQqtrcHG9zkhnvmE6yElgJsGTJkjGWJUka9tNHf/VQ26vqnbt5zBOA11bVNUnew26cKqqqNcAagKmpKe+qlqQxGvbmtSngLxhMhHc4cA5wHIPrCrt7bWETsKmqrmnWL2UQEncnOQygedyym79XkjSi3fmSnROq6qcASd4GXFJVr9rdA1bV95PcmeTYqroFWAbc1PysAFY3j5ft7u+WJI1m2FBYAtw/bf1+Bh8dna3XAhcl2Rf4DnAWg1HLuiRnA3cwuBdCkjRBw4bCx4Brk3yGwX0FZwAfne1Bq+qbPPiGuO2WzfZ3SpJGN+ynjy5I8jng95ums6rq+u7KkiT1YdgLzQALgXuq6j3ApiRHdVSTJKknw97RfD6DG8nOa5oeBny8q6IkSf0YdqRwBnAa8HOAqrqL0aa5kCTNQcOGwv1VVTTTZyc5oLuSJEl9GTYU1iV5P3BgklcDX8Qv3JGkvc4uP32UJAymtH48cA9wLPDWqrqq49okSRO2y1Coqkry2ap6KmAQSNJebNjTR19L8rROK5Ek9W7YO5qfA5yTZCODTyCFwSDiSV0VJkmavIcMhSRLquoO4HkTqkeS1KNdjRQ+y2B21O8m+VRVvXgCNUmSerKrawqZtnx0l4VIkvq3q1ConSxLkvZCuzp99OQk9zAYMezfLMOvLzQ/qtPqJEkT9ZChUFX7TKoQSVL/dmfqbEnSXs5QkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUqu3UEiyT5Lrk/x7s35wkquS3No8HtRXbZI0Xw37JTtdeD2wAdg+f9Iq4OqqWp1kVbP+pr6Kk7T7lq66YtbP3bj61DFWotnqZaSQ5AjgVOCD05qXA2ub5bXA6RMuS5Lmvb5OH70b+BvgV9PaDq2qzQDN4yEzPTHJyiTrk6zfunVr54VK0nwy8VBI8gJgS1VdN5vnV9WaqpqqqqnFixePuTpJmt/6uKZwInBakucD+wGPSvJx4O4kh1XV5iSHAVt6qE2S5rWJjxSq6ryqOqKqlgJnAv9ZVS8HLgdWNLutAC6bdG2SNN/NpfsUVgMnJ7kVOLlZlyRNUJ8fSaWqvgR8qVn+IbCsz3okab6bSyMFSVLPDAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1ep37SJK286s85wZHCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKk1sRDIcmRSf4ryYYkNyZ5fdN+cJKrktzaPB406dokab7rY6SwDfjrqnoC8Azg3CTHAauAq6vqGODqZl2SNEETD4Wq2lxV32iWfwpsAA4HlgNrm93WAqdPujZJmu96/ZKdJEuBpwDXAIdW1WYYBEeSQ3bynJXASoAlS5ZMqNK9wyhfYiJpfujtQnOSRwCfAt5QVfcM+7yqWlNVU1U1tXjx4u4KlKR5qJdQSPIwBoFwUVV9umm+O8lhzfbDgC191CZJ81kfnz4KcCGwoareOW3T5cCKZnkFcNmka5Ok+a6PawonAn8GfCvJN5u2NwOrgXVJzgbuAF7SQ22SNK9NPBSq6qtAdrJ52SRrkSQ9mHc0S5JahoIkqWUoSJJahoIkqWUoSJJahoIkqdXr3Efafc5fJKlLjhQkSS1DQZLU8vRRDzwFJI3XKH+nNq4+dYyV7PkcKUiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKnlzWuS5jVvfHswRwqSpJahIElqefpolpy/SNKo/w7MxdNPjhQkSS1DQZLUMhQkSa05FwpJTklyS5Lbkqzqux5Jmk/m1IXmJPsA/wycDGwCvp7k8qq6qYvjebFYUp/m4j0Sc22k8HTgtqr6TlXdD3wSWN5zTZI0b8ypkQJwOHDntPVNwO9O3yHJSmBls/qzJLdMqLZxWQT8oO8iemYfDNgP9gHMsg/y9pGO+didbZhroZAZ2upBK1VrgDWTKWf8kqyvqqm+6+iTfTBgP9gHMPf6YK6dPtoEHDlt/Qjgrp5qkaR5Z66FwteBY5IclWRf4Ezg8p5rkqR5Y06dPqqqbUn+EvgCsA/woaq6seeyxm2PPfU1RvbBgP1gH8Ac64NU1a73kiTNC3Pt9JEkqUeGgiSpZSiM0a6m6EiyPMn/JPlmkvVJnjVt28Yk39q+bbKVj8+IfXBgkkuT3JxkQ5Lfm2z14zHbPkhybNO2/eeeJG+Y+AsYgxHfB29McmOSG5JcnGS/yVY/HiP2weub13/jxN8DVeXPGH4YXBj/NnA0sC/w38BxO+zzCH59HedJwM3Ttm0EFvX9Onrug7XAq5rlfYED+35Nk+6DHX7P94HH9v2aJtkHDG5gvR3Yv1lfB7yi79c04T44HrgBWMjgw0BfBI6ZVO2OFMZnl1N0VNXPqvlTBw5ghxvz9gKz7oMkjwJOAi5s9ru/qn4yqcLHaFzvg2XAt6vqu51W241R+2ABsH+SBQz+YdwT71UapQ+eAHytqn5RVduALwNnTKhuQ2GMZpqi4/Add0pyRpKbgSuAV07bVMCVSa5rpvLYE43SB0cDW4EPJ7k+yQeTHNB1wR0Y9X2w3ZnAxZ1U2L1Z90FVfQ94B3AHsBn4v6q6svOKx2+U98ENwElJHp1kIfB8HnxTb6cMhfHZ5RQdAFX1map6PHA68HfTNp1YVScAzwPOTXJSJ1V2a5Q+WACcAPxLVT0F+DmwJ06dPur7gObGzdOAS7oocAJm3QdJDmLwP+qjgMcAByR5eXeldmbWfVBVG4C3A1cBn2dw6mlbZ5XuwFAYn92aoqOqvgI8LsmiZv2u5nEL8BkGw889zSh9sAnYVFXXNJsvZRASe5qR3geN5wHfqKq7uymxc6P0wXOB26tqa1X9Evg08Mwui+3IqP8eXFhVJ1TVScCPgFu7LHY6Q2F8djlFR5LfSZJm+QQGF6B+mOSAJI9s2g8A/ojBEHJPM+s+qKrvA3cmObbZdRnQyfdodGzWfTBtl5ex5546gtH64A7gGUkWNtuXARsmWv14jPQ+SHJI87gEeBETfD/MqWku9mS1kyk6kpzTbH8f8GLgz5P8ErgXeGlVVZJDgc80748FwCeq6vO9vJARjNIHza94LXBR85foO8BZE38RIxq1D5pzyCcDr+nlBYzBiH1wTZJLgW8wOGVyPXNsGohhjOHvwqeSPBr4JXBuVf14UrU7zYUkqeXpI0lSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlS6/8BgNJB/YWN/UIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.Series(deltas).plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
