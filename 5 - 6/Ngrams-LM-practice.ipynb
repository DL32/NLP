{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Read in the review corpus as simple counts.\n",
    "Use vector operations to find out \n",
    "- what the 5 most frequent words are in `X`\n",
    "- in how many different documents the word `delivery` occurs\n",
    "- what percentage of the overall corpus that number corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('service', 13137),\n",
       " ('00', 11926),\n",
       " ('time', 10679),\n",
       " ('great', 10517),\n",
       " ('order', 10353)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/Tiziano/Desktop/BOCCONI/2nd semester/NLP/data/reviews.full.tsv', sep='\\t', nrows=50000)\n",
    "documents = df.text.tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=0.001,\n",
    "                             max_df=0.75,\n",
    "                             stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "totals = X.sum(axis=0).A1\n",
    "\n",
    "counts = dict(zip(vectorizer.get_feature_names(),totals))\n",
    "c = Counter()\n",
    "c.update(counts)\n",
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0b1b47e98dd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdelivery_id\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'delivery'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelivery_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3639"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Extract **only** the bigrams (no unigrams) from Moby Dick and find the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [line.strip() for line in open('C:/Users/Tiziano/Desktop/BOCCONI/2nd semester/NLP/data/Moby_Dick.txt', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sperm whale', 143.4257711546659),\n",
       " ('white whale', 89.70019192803562),\n",
       " ('old man', 73.30125164382476),\n",
       " ('moby dick', 68.84065167956459),\n",
       " ('captain ahab', 53.91182956167341),\n",
       " ('right whale', 46.43710024527232),\n",
       " ('mast head', 41.407494642618744),\n",
       " ('mast heads', 32.45161742235697),\n",
       " ('cried ahab', 31.403015769389288),\n",
       " ('whale ship', 29.26108151507456)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,2),\n",
    "                                   min_df=0.001, \n",
    "                                   max_df=0.75, \n",
    "                                   stop_words='english', \n",
    "                                   sublinear_tf=True)\n",
    "\n",
    "Xsm = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "totals = Xsm.sum(axis=0).A1\n",
    "\n",
    "counts = dict(zip(tfidf_vectorizer.get_feature_names(),totals))\n",
    "c = Counter()\n",
    "c.update(counts)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Extract **only** the bigrams (no unigrams) from the Tweets and find the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [line.strip() for line in open('C:/Users/Tiziano/Desktop/BOCCONI/2nd semester/NLP/data/tweets_en.txt', encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('don know', 441.33674617344366),\n",
       " ('ve got', 355.1172757008388),\n",
       " ('happy birthday', 313.41269269432405),\n",
       " ('feel like', 280.35740534535717),\n",
       " ('looks like', 266.8842499656539),\n",
       " ('looking forward', 263.82871529735576),\n",
       " ('don think', 249.34119916190286),\n",
       " ('good luck', 246.29561092990502),\n",
       " ('don want', 202.45801508144262),\n",
       " ('just got', 195.45030945198397)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,2),\n",
    "                                   min_df=0.001, \n",
    "                                   max_df=0.75, \n",
    "                                   stop_words='english', \n",
    "                                   sublinear_tf=True)\n",
    "\n",
    "Y = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "totals = Y.sum(axis=0).A1\n",
    "\n",
    "counts = dict(zip(tfidf_vectorizer.get_feature_names(),totals))\n",
    "c = Counter()\n",
    "c.update(counts)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Let's modify the `generate()` function of our Language Model to take any number of initial words.\n",
    "First, read in the corpus and collect the counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "smoothing = 0.001\n",
    "START = '_***_'\n",
    "STOP = '_STOP_'\n",
    "\n",
    "# map from (u, v) to w = (w|u,v)\n",
    "counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n",
    "\n",
    "# fit data on corpus\n",
    "corpus = [line.strip().split() for line in open('../data/moby_dick.txt')]\n",
    "\n",
    "# collect counts for MLE\n",
    "for sentence in corpus:\n",
    "    # include special tokens for start and the end of sentence\n",
    "    tokens = [START, START] + sentence + [STOP]\n",
    "    for u, v, w in nltk.ngrams(tokens, 3):\n",
    "        counts[(u, v)][w] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the ``generate`` and ``sample_next_word`` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    # *****\n",
    "    # change code accordingly\n",
    "    # *****\n",
    "    result = [START, START]\n",
    "    next_word = sample_next_word(result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != STOP:\n",
    "        next_word = sample_next_word(result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "    \n",
    "    return ' '.join(result[2:-1])\n",
    "\n",
    "def sample_next_word(u, v):\n",
    "    \"\"\"\n",
    "    sample a word w based on the history (u, v)\n",
    "    \"\"\"\n",
    "    # separate word and their counts into separate variables\n",
    "    keys, values = zip(*counts[(u, v)].items())\n",
    "    \n",
    "    # normalize the counts into a probability distribution\n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    \n",
    "    # this is the meat of the function\n",
    "    sample = np.random.multinomial(1, values) # pick one position\n",
    "    \n",
    "    return keys[np.argmax(sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate('Hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Extend the LM code above to *arbitray* $n$-gram sizes. Use another corpus to try it with $n=4$.\n",
    "\n",
    "It might be helpful to use a `class` for the LM, make the smoothing a parameter, `counts` a class property, and add a function `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
