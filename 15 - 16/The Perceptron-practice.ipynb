{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ad90604f-afeb-4baa-b940-c3a1f3de8891"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    ranges from  0 to 1\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    ranges from  0 to 1\n",
    "    '''\n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    # instantiating the class, initializing the weights and bias \n",
    "    def __init__(self, num_inputs):\n",
    "        self.w1 = np.random.random(num_inputs) * 0.01\n",
    "        self.b1 = 1\n",
    "    \n",
    "    # feedforward pass\n",
    "    def predict(self, X):\n",
    "        # compute activation for input layer\n",
    "        activation = np.dot(X, self.w1) + self.b1\n",
    "        # non-linear transform\n",
    "        fX = sigmoid(activation)\n",
    "        # check threshold: for sigmoid and relu, use 0.5, for tanh, use 0\n",
    "        y = np.where(fX >= 0.5, 1, -1)\n",
    "        return y\n",
    "    \n",
    "    # training step\n",
    "    def fit(self, train_data, train_labels, num_epochs=50):\n",
    "        # iterate over epochs\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            print(epoch, end=' ')\n",
    "            \n",
    "            # predict with current parameters\n",
    "            for (X, y) in zip(train_data, train_labels):\n",
    "                pred_label = self.predict(X)\n",
    "\n",
    "                # adjusting weights and bias\n",
    "                if pred_label != y:\n",
    "                    self.w1 = self.w1 + (X * y)\n",
    "                    self.b1 = self.b1 + y\n",
    "        \n",
    "        print()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some simple text data to predict. We will us sparse vectors and limit ourselves to the top 1500 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "import pandas as pd\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), \n",
    "                        min_df=0.01, \n",
    "                        max_df=0.7, \n",
    "                        sublinear_tf=True, \n",
    "                        stop_words='english')\n",
    "\n",
    "selector = SelectKBest(k=1500, score_func=chi2)\n",
    "\n",
    "# read in files (already preprocessed)\n",
    "train = pd.read_csv('../data/sa_train.csv')\n",
    "dev = pd.read_csv('../data/sa_test.csv')\n",
    "\n",
    "# transform and select training data\n",
    "X = tfidf.fit_transform(train.clean_text)\n",
    "y = train.output.map({'neg':-1, 'pos':1}).values\n",
    "X = selector.fit_transform(X, y).todense().A\n",
    "\n",
    "# transform and select dev data\n",
    "Z = selector.transform(tfidf.transform(dev.clean_text)).todense().A\n",
    "z = dev.output.map({'neg':-1, 'pos':1}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Create a Perceptron object and fit it on the data `X` and `y` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "p = Perceptron(num_inputs=1500)\n",
    "p.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = p.predict(Z)\n",
    "print(classification_report(z, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Add a learning rate to the Perceptron. Use a default of $0.25$. Check the effect on performance with various learning rates.\n",
    "\n",
    "Hint: you will have to edit the `_init_` function and the update in the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants\n",
    "\n",
    "Implement two common variants of the perceptron that can improve performance. \n",
    "\n",
    "Either copy over your code from above and modify it to produce the variants, or implement a version that takes a parameter to decide what variant to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Averaged Perceptron\n",
    "\n",
    "Often, the perceptron algorithm's final decision boundary on the training set will result in only mediocre performance on a test set. This is true both when the algorithm finds a line that perfectly separates the two classes (and thus obtains 100% accuracy on the training set) and when such a line cannot be found, perhaps because the classes are not linearly separable. \n",
    "\n",
    "When the classes are separable, the \"perfect\" line may pass too close to some instances, at the risk of misclassifying similar though not identical instances in the test set. In that case, we say the classifier is *overfit*, because it relies on quirks of the training set and cannot generalize to new instances. Ideally, the line should be some distance away from the instances, leaving a *margin*. \n",
    "\n",
    "When the classes are not separable, the perceptron algorithm still tries as hard as it can to find the line, even if it eventually must give up and return whatever the current line is. Thus the final decision boundary is not necessarily the best one seen during the training.  \n",
    "\n",
    "Rather than taking the final weight vector to classify new instances, we can use information from the entire training process. One way is to classify each new instance with *all* weight vectors we produced during training and then see how often it ends up with one class or the other. This is called the *voted* perceptron, and it is slightly messy. A more practical approximation is the **averaged** perceptron, which you will implement, where we use the average overall different weight vectors we have seen in training.\n",
    "\n",
    "Hint: Save *a copy* of the current weight vector, including bias term, each time the weight vector is updated. When the training process has been completed, produce a new weight vector as the average of all saved vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Pocket Perceptron\n",
    "\n",
    "Another variant is the **pocket** perceptron, which keeps the best weight vector it has seen during training in its \"pocket\" and returns that as the final weight vector.\n",
    "\n",
    "Hint: \"Best\" here means the weight vector with the highest classification accuracy on the training set. You need to implement a variant of `fit` that takes a dev set, its gold labels, and a scoring fucntion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Exercise 4\n",
    "\n",
    "So far, we have used a version of the **online** perceptron, which updates after each instance. This results in a lot of updates per epoch, and can be sensitive to the order of instances.\n",
    "Rather than updateing after *every* instance, we can update after $k$ instances. This variant is called the **batch** perceptron. If $k=1$, we have online learning, with $k=|X|$, we updates once after we have seen all instances. \n",
    "\n",
    "Change the `fit()` function to only update every $k$ instances. Instead of feature vectors, you can now operate on a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbpresent": {
   "slides": {
    "3e606cca-ddab-4b87-8295-f1cca7962a9d": {
     "id": "3e606cca-ddab-4b87-8295-f1cca7962a9d",
     "prev": "8921ba99-3b08-4f47-affd-b6f8fadaa8cd",
     "regions": {
      "cb08d0e1-9b2e-4940-95dc-76a8f9e75b03": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67ca55d4-28b9-410e-9921-08d943aa3151",
        "part": "whole"
       },
       "id": "cb08d0e1-9b2e-4940-95dc-76a8f9e75b03"
      }
     }
    },
    "8921ba99-3b08-4f47-affd-b6f8fadaa8cd": {
     "id": "8921ba99-3b08-4f47-affd-b6f8fadaa8cd",
     "prev": null,
     "regions": {
      "48e51b5b-c219-4946-bad8-4b170ff7da1b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ad90604f-afeb-4baa-b940-c3a1f3de8891",
        "part": "whole"
       },
       "id": "48e51b5b-c219-4946-bad8-4b170ff7da1b"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
