{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Add a minimum frequency threshold for words, and replace any that are below with the `-UNK-` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Implement a simple n-gram Logistic Regression model and evaluate its performance with `classification report`. Compare to the CNN performance when using the minimum vocab frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Prediction with (Bi-)LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We need to get some data. We use the same CoNLL reader function as we had for the Structured Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_name):\n",
    "    \"\"\"\n",
    "    read in a file with CoNLL format:\n",
    "    word1    tag1\n",
    "    word2    tag2\n",
    "    ...      ...\n",
    "    wordN    tagN\n",
    "\n",
    "    Sentences MUST be separated by newlines!\n",
    "    \"\"\"\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(file_name, encoding='utf-8') as conll:\n",
    "        for line in conll:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                word, tag = line.split('\\t')\n",
    "                current_words.append(word)\n",
    "                current_tags.append(tag)\n",
    "\n",
    "            else:\n",
    "                yield (current_words, current_tags)\n",
    "                current_words = []\n",
    "                current_tags = []\n",
    "\n",
    "    # if file does not end in newline (it should...), check whether there is an instance in the buffer\n",
    "    if current_tags != []:\n",
    "        yield (current_words, current_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do some bookkeeping: collect the set of known words and tags, map both words and tags into integer indices, and keep track of the mapping. We also need two special tokens: one for unknown words, the other one for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect known word tokens and tags\n",
    "wordset, tagset = set(), set()\n",
    "train_instances = [(words, tags) for (words, tags) in read_conll_file('../data/tiny_POS_train.data')]\n",
    "for (words, tags) in train_instances:\n",
    "    tagset.update(set(tags))\n",
    "    wordset.update(set(words))\n",
    "\n",
    "# map words and tags into ints\n",
    "PAD = '-PAD-'\n",
    "UNK = '-UNK-'\n",
    "word2int = {word: i + 2 for i, word in enumerate(sorted(wordset))}\n",
    "word2int[PAD] = 0  # special token for padding\n",
    "word2int[UNK] = 1  # special token for unknown words\n",
    " \n",
    "tag2int = {tag: i + 1 for i, tag in enumerate(sorted(tagset))}\n",
    "tag2int[PAD] = 0\n",
    "# to translate it back\n",
    "int2tag = {i:tag for tag, i in tag2int.items()}\n",
    "\n",
    "\n",
    "def convert2ints(instances):\n",
    "    result = []\n",
    "    for (words, tags) in instances:\n",
    "        # replace words with int, 1 for unknown words\n",
    "        word_ints = [word2int.get(word, 1) for word in words]\n",
    "        # replace tags with int\n",
    "        tag_ints = [tag2int[tag] for tag in tags]\n",
    "        result.append((word_ints, tag_ints))\n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Showtime', 'is', 'a', 'distant', 'No.', '0', 'to', 'Home', 'Box', 'Office', ',', 'and', 'in', 'May', 'filed', 'a', '$', '0.0', 'billion', 'antitrust', 'suit', 'against', 'Time', 'Warner', ',', 'charging', 'the', 'company', 'and', 'its', 'HBO', 'and', 'American', 'Television', 'cable', 'units', 'with', 'conspiring', 'to', 'monopolize', 'the', 'pay', 'TV', 'business', '.']\n",
      "[1789, 4586, 2131, 3496, 1433, 25, 6898, 929, 331, 1472, 20, 2336, 4422, 1282, 3917, 2131, 3, 31, 2627, 2357, 6697, 2255, 1949, 2054, 20, 2878, 6829, 3000, 2336, 4594, 875, 2336, 175, 1917, 2780, 7069, 7309, 3104, 6898, 5051, 6829, 5413, 1899, 2755, 23]\n",
      "['NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NUM', 'ADP', 'NOUN', 'NOUN', 'NOUN', '.', 'CONJ', 'ADP', 'NOUN', 'VERB', 'DET', '.', 'NUM', 'NUM', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', '.', 'VERB', 'DET', 'NOUN', 'CONJ', 'PRON', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'NOUN', 'NOUN', '.']\n",
      "[7, 11, 6, 2, 7, 8, 3, 7, 7, 7, 1, 5, 3, 7, 11, 6, 1, 8, 8, 2, 7, 3, 7, 7, 1, 11, 6, 7, 5, 9, 7, 5, 7, 7, 7, 7, 3, 11, 10, 11, 6, 7, 7, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "# get some test data\n",
    "test_instances = [(words, tags) for (words, tags) in read_conll_file('../data/tiny_POS_test.data')]\n",
    "\n",
    "# apply integer mapping\n",
    "train_instances_int = convert2ints(train_instances)\n",
    "test_instances_int = convert2ints(test_instances)\n",
    "\n",
    "# separate the words from the tags\n",
    "train_sentences, train_tags = zip(*train_instances_int) \n",
    "test_sentences, test_tags = zip(*test_instances_int) \n",
    "\n",
    "print(train_instances[0][0])\n",
    "print(train_sentences[0])\n",
    "print(train_instances[0][1])\n",
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Instead of the longest sentence, find the 90th percentile of all training sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "Get the large train, dev, and test data sets for CoNLL, apply the preprocessing steps and run the model on them. What performance do you get? How does it compare to the structured perceptron in terms of performance and speed (both training and test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
