{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Representations\n",
    "\n",
    "## *\"I know words. I have the best words!\"*\n",
    "    - Noam Chomsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense Distributed Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T17:48:18.980145Z",
     "start_time": "2021-02-25T17:48:16.733594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Prices change daily and if you want to really research the price continually at many different sites , I have found cheaper cars elsewhere . However , if you don ' t have a lot of time to research the price , this site has always been among the top three ( e . g ., cheapest ) of the ten sites I use to reserve a car .\", 'and the fact that they will match other companies is awesome !!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/reviews.full.tsv', sep='\\t', nrows=100000)\n",
    "documents = df.text.tolist()\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word embeddings with `Word2vec`\n",
    "\n",
    "### `Word2Vec` parameters:\n",
    "\n",
    "- **size** (int, optional) – Dimensionality of the word vectors.\n",
    "\n",
    "- **window** (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "- **sample** (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "\n",
    "- **iter** (int, optional) – Number of iterations (epochs) over the corpus.\n",
    "\n",
    "- **negative** (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "\n",
    "- **min_count** (int, optional) – Ignores all words with total frequency lower than this.\n",
    "\n",
    "- **hs** ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "\n",
    "- **workers** (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import FAST_VERSION\n",
    "\n",
    "corpus = [document.split() for document in documents]\n",
    "\n",
    "# initialize model\n",
    "w2v_model = Word2Vec(size=100, # vector size\n",
    "                     window=15, # window for sampling\n",
    "                     sample=0.0001, # subsampling rate\n",
    "                     iter=200, # iterations\n",
    "                     negative=5, # negative samples\n",
    "                     min_count=100, # minimum threshold\n",
    "                     workers=-1, # parallelize to all cores\n",
    "                     hs=0 # no hierarchical softmax\n",
    ")\n",
    "\n",
    "# build the vocabulary\n",
    "w2v_model.build_vocab(corpus)\n",
    "\n",
    "# train the model\n",
    "w2v_model.train(corpus, \n",
    "                total_examples=w2v_model.corpus_count, \n",
    "                epochs=w2v_model.epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can use the embeddings of the model via the `wv` (word vector) property. Each word maps to its vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3967641e-03,  3.1956586e-03,  4.1554654e-03, -3.6858404e-03,\n",
       "        2.0383890e-03, -3.3349530e-03,  3.9955527e-03, -3.2048434e-04,\n",
       "        3.1501739e-03,  1.7654008e-03, -3.3951167e-03,  1.7206426e-03,\n",
       "        4.4660675e-03, -1.4680600e-03, -1.8679150e-03, -3.3995991e-03,\n",
       "        2.8441187e-03, -4.6684514e-03, -7.2946388e-04,  4.6961126e-03,\n",
       "        3.0271111e-03,  2.5028719e-03,  1.1871291e-03,  3.2459085e-03,\n",
       "       -4.2324271e-03,  1.1746244e-03,  3.4704435e-04, -3.7060806e-03,\n",
       "        9.2344050e-04,  2.0046136e-03,  2.4872362e-03, -3.3609455e-03,\n",
       "       -3.0028156e-03,  8.7623222e-04, -5.6574220e-04, -8.2645420e-04,\n",
       "        2.1637853e-03,  4.0302886e-05, -3.4664737e-03, -4.2506945e-03,\n",
       "       -1.6045283e-03,  2.6106955e-03, -3.7266579e-03, -2.0738917e-03,\n",
       "        6.9069810e-04,  2.6558433e-03, -3.1873595e-03, -3.6215123e-03,\n",
       "        2.0043796e-03, -3.2183380e-04,  7.3239760e-04, -3.7281048e-03,\n",
       "       -2.6600130e-03, -3.5996456e-03,  6.6113472e-04,  2.3121831e-03,\n",
       "       -4.4178776e-03,  7.6116397e-05, -4.8095910e-03,  3.3757857e-03,\n",
       "        2.6241268e-03,  4.5037018e-03, -3.0940687e-03,  2.2090084e-03,\n",
       "        5.9001322e-04, -3.6687506e-04, -2.0383329e-03, -1.7236327e-03,\n",
       "       -6.7733397e-04, -3.5910108e-03,  2.6564801e-03,  2.7808121e-03,\n",
       "        1.5755479e-03,  3.8098968e-03,  1.1120713e-03, -3.3752921e-03,\n",
       "        3.6226269e-03,  2.5769363e-03, -3.9791646e-03,  6.2162767e-04,\n",
       "       -4.1566365e-03,  1.7242500e-03, -6.8124861e-04,  3.5195891e-03,\n",
       "       -4.3530934e-03,  1.3792738e-03, -3.2076985e-03,  5.2074791e-04,\n",
       "        4.5014252e-03, -2.6571960e-03,  1.6562482e-04, -5.7858293e-04,\n",
       "       -2.8768566e-03,  3.7294095e-03, -8.3133922e-04,  4.3754242e-03,\n",
       "        2.7978742e-03, -1.7993685e-03,  2.9328081e-03,  4.6356316e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['delivery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Such', 0.37224239110946655),\n",
       " ('join', 0.3092310428619385),\n",
       " ('building', 0.2821229100227356),\n",
       " ('Needed', 0.2772734761238098),\n",
       " ('mails', 0.2770233750343323),\n",
       " ('messed', 0.2687864303588867),\n",
       " ('almost', 0.26552754640579224),\n",
       " ('bars', 0.26520830392837524),\n",
       " ('individual', 0.26351213455200195),\n",
       " ('trust', 0.26270318031311035)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(['delivery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also solve the analogy tasks by defining positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jewelry', 0.4413125514984131),\n",
       " ('finding', 0.33475425839424133),\n",
       " ('beautiful', 0.32356536388397217)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# birthday - present + husband => birthday:present as husband:?\n",
    "w2v_model.wv.most_similar(positive=['birthday', 'husband'], negative=['present'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.041342914\n",
      "[('speed', 0.3675766587257385), ('She', 0.33542460203170776), ('hoping', 0.31432610750198364)]\n"
     ]
    }
   ],
   "source": [
    "word1 = \"Cheapest\"\n",
    "word2 = \"friendly\"\n",
    "\n",
    "# retrieve the actual vector\n",
    "# print(w2v_model.wv[word1])\n",
    "\n",
    "# compare\n",
    "print(w2v_model.wv.similarity(word1, word2))\n",
    "\n",
    "# get the 3 most similar words\n",
    "print(w2v_model.wv.most_similar(word1, topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document embeddings with `Doc2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>category</th>\n",
       "      <th>uid</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Car Rental</td>\n",
       "      <td>899881</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>Prices change daily and if you want to really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Fitness &amp; Nutrition</td>\n",
       "      <td>828184</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>and the fact that they will match other compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Electronic Payment</td>\n",
       "      <td>1698375</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>Used Paypal for my buying and selling for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>3324079</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>I ' ve made two purchases on CJ ' s for Fallou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>719816</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>I was very happy with the diamond that I order...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score             category      uid gender  age  \\\n",
       "0      5           Car Rental   899881      F   50   \n",
       "1      5  Fitness & Nutrition   828184      M   32   \n",
       "2      5   Electronic Payment  1698375      M   48   \n",
       "3      5               Gaming  3324079      M   29   \n",
       "4      4              Jewelry   719816      F   29   \n",
       "\n",
       "                                                text  \n",
       "0  Prices change daily and if you want to really ...  \n",
       "1  and the fact that they will match other compan...  \n",
       "2  Used Paypal for my buying and selling for the ...  \n",
       "3  I ' ve made two purchases on CJ ' s for Fallou...  \n",
       "4  I was very happy with the diamond that I order...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Doc2Vec` parameters:\n",
    "\n",
    "- **vector_size** (int, optional) – Dimensionality of the feature vectors.\n",
    "\n",
    "- **window** (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "- **hs** ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "\n",
    "- **sample** (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "\n",
    "- **negative** (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "\n",
    "- **min_count** (int, optional) – Ignores all words with total frequency lower than this.\n",
    "\n",
    "- **workers** (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "- **epochs** (int, optional) – Number of iterations (epochs) over the corpus.\n",
    "\n",
    "- **dm** ({1,0}, optional) – Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "\n",
    "- **dbow_words** ({1,0}, optional) – If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW doc-vector training; If 0, only trains doc-vectors (faster).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface for Doc2Vec is almost the same as for Word2vec. The main difference is that we need to give it a different input format, namely `TaggedDocument` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# create a list of TaggedDocument objects\n",
    "corpus = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "    label = row[1].score\n",
    "    text = row[1].text\n",
    "    corpus.append(TaggedDocument(words=text.split(), \n",
    "                                 tags=[str(label)]))\n",
    "\n",
    "print('done')\n",
    "# initialize model\n",
    "d2v_model = Doc2Vec(vector_size=100, \n",
    "                    window=15,\n",
    "                    hs=0,\n",
    "                    sample=0.000001,\n",
    "                    negative=5,\n",
    "                    min_count=100,\n",
    "                    workers=-1,\n",
    "                    epochs=500,\n",
    "                    dm=0, \n",
    "                    dbow_words=1)\n",
    "\n",
    "# build the vocabulary\n",
    "d2v_model.build_vocab(corpus)\n",
    "\n",
    "# train the model\n",
    "d2v_model.train(corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now look at the elements. Doc2vec stores the words as before in `wv`, and the document representations in `docvecs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': Doctag(offset=0, word_count=4205492, doc_count=78827),\n",
       " '4': Doctag(offset=1, word_count=604853, doc_count=9164),\n",
       " '1': Doctag(offset=2, word_count=1205430, doc_count=7316),\n",
       " '2': Doctag(offset=3, word_count=301478, doc_count=2197),\n",
       " '3': Doctag(offset=4, word_count=254820, doc_count=2496)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.doctags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now similarly compare document categories to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 0.0817607194185257), ('4', 0.07102358341217041), ('5', 0.030553873628377914), ('2', -0.04616723209619522)]\n"
     ]
    }
   ],
   "source": [
    "target_doc = '1'\n",
    "\n",
    "similar_docs = d2v_model.docvecs.most_similar(target_doc, topn=5)\n",
    "print(similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
